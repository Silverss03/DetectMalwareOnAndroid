import torch
import torch.nn as nn
import torch.nn.functional as F

class RGATLayer(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_heads):
        super(RGATLayer, self).__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads

        # Linear transformations for node features
        self.linear = nn.Linear(input_dim, hidden_dim * num_heads, bias=False)
        # Adjacency matrix transformation
        self.attention_weights = nn.Parameter(torch.Tensor(num_heads, 2 * hidden_dim))
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.linear.weight.data)
        nn.init.xavier_uniform_(self.attention_weights.data)

    def forward(self, node_features, adjacency_matrix):
        # Linear transformation of node features
        h = self.linear(node_features).view(-1, self.num_heads, self.hidden_dim)
        # Attention mechanism
        a_input = torch.cat([h.unsqueeze(2).expand(-1, -1, adjacency_matrix.size(1), -1),
                             h.unsqueeze(1).expand(-1, adjacency_matrix.size(1), -1, -1)], dim=-1)
        attention_scores = torch.einsum('bijh,jkh->bijk', a_input, self.attention_weights).squeeze(-1)
        attention_scores = F.leaky_relu(attention_scores, negative_slope=0.2)
        attention_scores = F.softmax(attention_scores.masked_fill(adjacency_matrix.unsqueeze(1) == 0, float('-inf')), dim=-1)
        # Aggregation
        h_prime = torch.einsum('bijh,bij->bjh', (h, attention_scores))
        return h_prime.view(-1, self.num_heads * self.hidden_dim)

class RGATModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_heads):
        super(RGATModel, self).__init__()
        self.rgat_layer = RGATLayer(input_dim, hidden_dim, num_heads)

    def forward(self, node_features, adjacency_matrix):
        return self.rgat_layer(node_features, adjacency_matrix)

# Example usage:
# Initialize RGAT model with input dimension, hidden dimension, and number of attention heads
input_dim = 100  # Dimension of input node features
hidden_dim = 64  # Dimension of hidden layers
num_heads = 10   # Number of attention heads
rgat_model = RGATModel(input_dim, hidden_dim, num_heads)

# Assuming node_features is the embedding of each function and adjacency_matrix is the adjacency matrix of the FCG
node_features = torch.randn(10, input_dim)  # Example: 10 nodes with input_dim features each
adjacency_matrix = torch.randint(2, (10, 10))  #tạo ma trận kề chỉ có các số 0 hoặc 1, có kích thước 10*10
output = rgat_model(node_features, adjacency_matrix)
print(output.size())  # Print the size of the output vector representation
