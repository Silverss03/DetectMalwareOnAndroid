{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM51KJJeDYRPyO1QFLbF580",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theducdev/DetectMalwareOnAndroid/blob/master/gcn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /sample_data\n",
        "!python train.py"
      ],
      "metadata": {
        "id": "uxBw4AWHnU9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7734871c-513d-4842-9ef2-88c162e837af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/sample_data'\n",
            "/content\n",
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "Training with DGL built-in GraphConv module.\n",
            "Downloading /root/.dgl/cora_v2.zip from https://data.dgl.ai/dataset/cora_v2.zip...\n",
            "/root/.dgl/cora_v2.zip: 100% 132k/132k [00:00<00:00, 214kB/s]\n",
            "Extracting file to /root/.dgl/cora_v2_d697a464\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done saving data into cached files.\n",
            "Training...\n",
            "Epoch 00000 | Loss 1.9460 | Accuracy 0.2580 \n",
            "Epoch 00001 | Loss 1.9408 | Accuracy 0.2820 \n",
            "Epoch 00002 | Loss 1.9344 | Accuracy 0.5220 \n",
            "Epoch 00003 | Loss 1.9255 | Accuracy 0.5060 \n",
            "Epoch 00004 | Loss 1.9150 | Accuracy 0.5400 \n",
            "Epoch 00005 | Loss 1.9040 | Accuracy 0.6120 \n",
            "Epoch 00006 | Loss 1.8963 | Accuracy 0.6540 \n",
            "Epoch 00007 | Loss 1.8847 | Accuracy 0.6340 \n",
            "Epoch 00008 | Loss 1.8727 | Accuracy 0.6240 \n",
            "Epoch 00009 | Loss 1.8589 | Accuracy 0.6320 \n",
            "Epoch 00010 | Loss 1.8377 | Accuracy 0.6440 \n",
            "Epoch 00011 | Loss 1.8335 | Accuracy 0.6460 \n",
            "Epoch 00012 | Loss 1.8086 | Accuracy 0.6660 \n",
            "Epoch 00013 | Loss 1.7971 | Accuracy 0.6780 \n",
            "Epoch 00014 | Loss 1.7876 | Accuracy 0.6940 \n",
            "Epoch 00015 | Loss 1.7556 | Accuracy 0.7060 \n",
            "Epoch 00016 | Loss 1.7542 | Accuracy 0.7160 \n",
            "Epoch 00017 | Loss 1.7375 | Accuracy 0.7180 \n",
            "Epoch 00018 | Loss 1.7115 | Accuracy 0.7200 \n",
            "Epoch 00019 | Loss 1.7075 | Accuracy 0.7220 \n",
            "Epoch 00020 | Loss 1.6833 | Accuracy 0.7220 \n",
            "Epoch 00021 | Loss 1.6429 | Accuracy 0.7220 \n",
            "Epoch 00022 | Loss 1.6414 | Accuracy 0.7180 \n",
            "Epoch 00023 | Loss 1.6201 | Accuracy 0.7180 \n",
            "Epoch 00024 | Loss 1.5954 | Accuracy 0.7220 \n",
            "Epoch 00025 | Loss 1.5823 | Accuracy 0.7240 \n",
            "Epoch 00026 | Loss 1.5584 | Accuracy 0.7260 \n",
            "Epoch 00027 | Loss 1.5414 | Accuracy 0.7340 \n",
            "Epoch 00028 | Loss 1.5141 | Accuracy 0.7320 \n",
            "Epoch 00029 | Loss 1.4941 | Accuracy 0.7380 \n",
            "Epoch 00030 | Loss 1.4799 | Accuracy 0.7480 \n",
            "Epoch 00031 | Loss 1.4690 | Accuracy 0.7540 \n",
            "Epoch 00032 | Loss 1.4707 | Accuracy 0.7600 \n",
            "Epoch 00033 | Loss 1.4124 | Accuracy 0.7620 \n",
            "Epoch 00034 | Loss 1.3756 | Accuracy 0.7620 \n",
            "Epoch 00035 | Loss 1.3496 | Accuracy 0.7600 \n",
            "Epoch 00036 | Loss 1.3474 | Accuracy 0.7640 \n",
            "Epoch 00037 | Loss 1.3525 | Accuracy 0.7700 \n",
            "Epoch 00038 | Loss 1.2866 | Accuracy 0.7780 \n",
            "Epoch 00039 | Loss 1.2944 | Accuracy 0.7780 \n",
            "Epoch 00040 | Loss 1.2519 | Accuracy 0.7780 \n",
            "Epoch 00041 | Loss 1.2174 | Accuracy 0.7760 \n",
            "Epoch 00042 | Loss 1.2218 | Accuracy 0.7800 \n",
            "Epoch 00043 | Loss 1.1668 | Accuracy 0.7840 \n",
            "Epoch 00044 | Loss 1.1586 | Accuracy 0.7820 \n",
            "Epoch 00045 | Loss 1.1386 | Accuracy 0.7880 \n",
            "Epoch 00046 | Loss 1.1167 | Accuracy 0.7900 \n",
            "Epoch 00047 | Loss 1.0981 | Accuracy 0.7880 \n",
            "Epoch 00048 | Loss 1.0531 | Accuracy 0.7900 \n",
            "Epoch 00049 | Loss 1.0579 | Accuracy 0.7880 \n",
            "Epoch 00050 | Loss 1.0634 | Accuracy 0.7880 \n",
            "Epoch 00051 | Loss 1.0452 | Accuracy 0.7900 \n",
            "Epoch 00052 | Loss 1.0201 | Accuracy 0.7900 \n",
            "Epoch 00053 | Loss 0.9585 | Accuracy 0.7880 \n",
            "Epoch 00054 | Loss 0.9827 | Accuracy 0.7880 \n",
            "Epoch 00055 | Loss 0.9185 | Accuracy 0.7860 \n",
            "Epoch 00056 | Loss 0.9193 | Accuracy 0.7860 \n",
            "Epoch 00057 | Loss 0.9067 | Accuracy 0.7880 \n",
            "Epoch 00058 | Loss 0.8536 | Accuracy 0.7900 \n",
            "Epoch 00059 | Loss 0.8618 | Accuracy 0.7940 \n",
            "Epoch 00060 | Loss 0.8844 | Accuracy 0.7980 \n",
            "Epoch 00061 | Loss 0.8554 | Accuracy 0.8020 \n",
            "Epoch 00062 | Loss 0.8171 | Accuracy 0.8040 \n",
            "Epoch 00063 | Loss 0.8170 | Accuracy 0.8040 \n",
            "Epoch 00064 | Loss 0.7506 | Accuracy 0.8040 \n",
            "Epoch 00065 | Loss 0.7670 | Accuracy 0.7980 \n",
            "Epoch 00066 | Loss 0.7831 | Accuracy 0.7980 \n",
            "Epoch 00067 | Loss 0.8160 | Accuracy 0.7960 \n",
            "Epoch 00068 | Loss 0.7763 | Accuracy 0.7920 \n",
            "Epoch 00069 | Loss 0.7001 | Accuracy 0.7960 \n",
            "Epoch 00070 | Loss 0.7067 | Accuracy 0.7960 \n",
            "Epoch 00071 | Loss 0.7396 | Accuracy 0.7960 \n",
            "Epoch 00072 | Loss 0.6909 | Accuracy 0.7940 \n",
            "Epoch 00073 | Loss 0.7085 | Accuracy 0.7980 \n",
            "Epoch 00074 | Loss 0.7014 | Accuracy 0.8020 \n",
            "Epoch 00075 | Loss 0.6693 | Accuracy 0.8000 \n",
            "Epoch 00076 | Loss 0.6591 | Accuracy 0.7980 \n",
            "Epoch 00077 | Loss 0.6662 | Accuracy 0.8000 \n",
            "Epoch 00078 | Loss 0.6366 | Accuracy 0.7960 \n",
            "Epoch 00079 | Loss 0.6412 | Accuracy 0.7940 \n",
            "Epoch 00080 | Loss 0.6233 | Accuracy 0.7900 \n",
            "Epoch 00081 | Loss 0.5951 | Accuracy 0.7900 \n",
            "Epoch 00082 | Loss 0.6352 | Accuracy 0.7900 \n",
            "Epoch 00083 | Loss 0.6142 | Accuracy 0.7880 \n",
            "Epoch 00084 | Loss 0.5780 | Accuracy 0.7880 \n",
            "Epoch 00085 | Loss 0.5663 | Accuracy 0.7900 \n",
            "Epoch 00086 | Loss 0.5665 | Accuracy 0.7900 \n",
            "Epoch 00087 | Loss 0.5855 | Accuracy 0.7920 \n",
            "Epoch 00088 | Loss 0.5740 | Accuracy 0.7920 \n",
            "Epoch 00089 | Loss 0.5420 | Accuracy 0.7940 \n",
            "Epoch 00090 | Loss 0.5806 | Accuracy 0.7940 \n",
            "Epoch 00091 | Loss 0.5654 | Accuracy 0.7940 \n",
            "Epoch 00092 | Loss 0.5474 | Accuracy 0.7900 \n",
            "Epoch 00093 | Loss 0.5430 | Accuracy 0.7880 \n",
            "Epoch 00094 | Loss 0.5347 | Accuracy 0.7920 \n",
            "Epoch 00095 | Loss 0.4984 | Accuracy 0.7980 \n",
            "Epoch 00096 | Loss 0.5339 | Accuracy 0.7960 \n",
            "Epoch 00097 | Loss 0.5119 | Accuracy 0.8000 \n",
            "Epoch 00098 | Loss 0.5284 | Accuracy 0.8020 \n",
            "Epoch 00099 | Loss 0.4978 | Accuracy 0.7980 \n",
            "Epoch 00100 | Loss 0.5154 | Accuracy 0.7940 \n",
            "Epoch 00101 | Loss 0.5008 | Accuracy 0.7940 \n",
            "Epoch 00102 | Loss 0.5093 | Accuracy 0.7940 \n",
            "Epoch 00103 | Loss 0.5165 | Accuracy 0.7940 \n",
            "Epoch 00104 | Loss 0.4929 | Accuracy 0.7940 \n",
            "Epoch 00105 | Loss 0.4595 | Accuracy 0.7960 \n",
            "Epoch 00106 | Loss 0.4864 | Accuracy 0.7960 \n",
            "Epoch 00107 | Loss 0.4442 | Accuracy 0.7940 \n",
            "Epoch 00108 | Loss 0.4642 | Accuracy 0.7960 \n",
            "Epoch 00109 | Loss 0.4667 | Accuracy 0.7980 \n",
            "Epoch 00110 | Loss 0.5144 | Accuracy 0.8020 \n",
            "Epoch 00111 | Loss 0.4534 | Accuracy 0.7980 \n",
            "Epoch 00112 | Loss 0.4677 | Accuracy 0.7900 \n",
            "Epoch 00113 | Loss 0.4654 | Accuracy 0.7920 \n",
            "Epoch 00114 | Loss 0.4405 | Accuracy 0.7920 \n",
            "Epoch 00115 | Loss 0.4315 | Accuracy 0.7940 \n",
            "Epoch 00116 | Loss 0.4404 | Accuracy 0.7940 \n",
            "Epoch 00117 | Loss 0.4437 | Accuracy 0.7960 \n",
            "Epoch 00118 | Loss 0.4769 | Accuracy 0.7960 \n",
            "Epoch 00119 | Loss 0.4534 | Accuracy 0.7980 \n",
            "Epoch 00120 | Loss 0.4451 | Accuracy 0.8020 \n",
            "Epoch 00121 | Loss 0.4388 | Accuracy 0.8040 \n",
            "Epoch 00122 | Loss 0.4080 | Accuracy 0.8000 \n",
            "Epoch 00123 | Loss 0.4592 | Accuracy 0.7880 \n",
            "Epoch 00124 | Loss 0.4055 | Accuracy 0.7900 \n",
            "Epoch 00125 | Loss 0.4323 | Accuracy 0.7900 \n",
            "Epoch 00126 | Loss 0.4274 | Accuracy 0.7900 \n",
            "Epoch 00127 | Loss 0.4149 | Accuracy 0.7860 \n",
            "Epoch 00128 | Loss 0.4169 | Accuracy 0.7880 \n",
            "Epoch 00129 | Loss 0.4383 | Accuracy 0.7900 \n",
            "Epoch 00130 | Loss 0.4308 | Accuracy 0.7900 \n",
            "Epoch 00131 | Loss 0.4083 | Accuracy 0.7980 \n",
            "Epoch 00132 | Loss 0.4206 | Accuracy 0.8020 \n",
            "Epoch 00133 | Loss 0.3742 | Accuracy 0.8060 \n",
            "Epoch 00134 | Loss 0.3897 | Accuracy 0.8040 \n",
            "Epoch 00135 | Loss 0.4026 | Accuracy 0.8020 \n",
            "Epoch 00136 | Loss 0.4236 | Accuracy 0.7960 \n",
            "Epoch 00137 | Loss 0.4024 | Accuracy 0.7940 \n",
            "Epoch 00138 | Loss 0.3812 | Accuracy 0.7900 \n",
            "Epoch 00139 | Loss 0.3679 | Accuracy 0.7900 \n",
            "Epoch 00140 | Loss 0.4092 | Accuracy 0.7940 \n",
            "Epoch 00141 | Loss 0.4027 | Accuracy 0.7920 \n",
            "Epoch 00142 | Loss 0.4090 | Accuracy 0.7860 \n",
            "Epoch 00143 | Loss 0.3796 | Accuracy 0.7920 \n",
            "Epoch 00144 | Loss 0.3658 | Accuracy 0.7920 \n",
            "Epoch 00145 | Loss 0.3710 | Accuracy 0.7940 \n",
            "Epoch 00146 | Loss 0.3580 | Accuracy 0.8040 \n",
            "Epoch 00147 | Loss 0.3814 | Accuracy 0.8060 \n",
            "Epoch 00148 | Loss 0.3525 | Accuracy 0.8040 \n",
            "Epoch 00149 | Loss 0.3599 | Accuracy 0.8000 \n",
            "Epoch 00150 | Loss 0.3726 | Accuracy 0.8020 \n",
            "Epoch 00151 | Loss 0.3750 | Accuracy 0.8040 \n",
            "Epoch 00152 | Loss 0.3811 | Accuracy 0.8000 \n",
            "Epoch 00153 | Loss 0.3505 | Accuracy 0.8000 \n",
            "Epoch 00154 | Loss 0.3853 | Accuracy 0.7940 \n",
            "Epoch 00155 | Loss 0.3734 | Accuracy 0.7900 \n",
            "Epoch 00156 | Loss 0.3782 | Accuracy 0.7880 \n",
            "Epoch 00157 | Loss 0.3506 | Accuracy 0.7880 \n",
            "Epoch 00158 | Loss 0.3708 | Accuracy 0.7880 \n",
            "Epoch 00159 | Loss 0.3352 | Accuracy 0.7920 \n",
            "Epoch 00160 | Loss 0.3273 | Accuracy 0.7960 \n",
            "Epoch 00161 | Loss 0.3602 | Accuracy 0.8000 \n",
            "Epoch 00162 | Loss 0.3362 | Accuracy 0.8020 \n",
            "Epoch 00163 | Loss 0.3009 | Accuracy 0.8040 \n",
            "Epoch 00164 | Loss 0.3563 | Accuracy 0.8020 \n",
            "Epoch 00165 | Loss 0.3477 | Accuracy 0.7980 \n",
            "Epoch 00166 | Loss 0.3309 | Accuracy 0.7940 \n",
            "Epoch 00167 | Loss 0.3326 | Accuracy 0.7960 \n",
            "Epoch 00168 | Loss 0.3524 | Accuracy 0.7960 \n",
            "Epoch 00169 | Loss 0.3433 | Accuracy 0.7960 \n",
            "Epoch 00170 | Loss 0.3474 | Accuracy 0.8000 \n",
            "Epoch 00171 | Loss 0.3816 | Accuracy 0.7980 \n",
            "Epoch 00172 | Loss 0.3114 | Accuracy 0.7960 \n",
            "Epoch 00173 | Loss 0.3323 | Accuracy 0.7980 \n",
            "Epoch 00174 | Loss 0.3595 | Accuracy 0.7940 \n",
            "Epoch 00175 | Loss 0.3004 | Accuracy 0.7960 \n",
            "Epoch 00176 | Loss 0.3324 | Accuracy 0.7980 \n",
            "Epoch 00177 | Loss 0.2962 | Accuracy 0.7940 \n",
            "Epoch 00178 | Loss 0.3336 | Accuracy 0.7920 \n",
            "Epoch 00179 | Loss 0.3155 | Accuracy 0.7940 \n",
            "Epoch 00180 | Loss 0.3325 | Accuracy 0.7960 \n",
            "Epoch 00181 | Loss 0.3140 | Accuracy 0.7960 \n",
            "Epoch 00182 | Loss 0.3315 | Accuracy 0.7960 \n",
            "Epoch 00183 | Loss 0.3006 | Accuracy 0.7980 \n",
            "Epoch 00184 | Loss 0.3109 | Accuracy 0.7980 \n",
            "Epoch 00185 | Loss 0.3321 | Accuracy 0.7980 \n",
            "Epoch 00186 | Loss 0.3080 | Accuracy 0.7980 \n",
            "Epoch 00187 | Loss 0.3033 | Accuracy 0.8000 \n",
            "Epoch 00188 | Loss 0.3130 | Accuracy 0.8000 \n",
            "Epoch 00189 | Loss 0.3236 | Accuracy 0.8000 \n",
            "Epoch 00190 | Loss 0.3322 | Accuracy 0.7980 \n",
            "Epoch 00191 | Loss 0.2809 | Accuracy 0.7980 \n",
            "Epoch 00192 | Loss 0.3613 | Accuracy 0.7940 \n",
            "Epoch 00193 | Loss 0.2951 | Accuracy 0.7960 \n",
            "Epoch 00194 | Loss 0.2995 | Accuracy 0.7960 \n",
            "Epoch 00195 | Loss 0.2793 | Accuracy 0.7960 \n",
            "Epoch 00196 | Loss 0.3167 | Accuracy 0.7980 \n",
            "Epoch 00197 | Loss 0.3029 | Accuracy 0.7940 \n",
            "Epoch 00198 | Loss 0.3099 | Accuracy 0.7980 \n",
            "Epoch 00199 | Loss 0.2793 | Accuracy 0.7980 \n",
            "Testing...\n",
            "Test accuracy 0.8080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt5lkN52xQMl",
        "outputId": "173a4dcf-7c8d-48b1-f241-8e41652e14c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl\n",
            "  Downloading dgl-2.1.0-cp310-cp310-manylinux1_x86_64.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.2.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: torchdata>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (0.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2024.2.2)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata>=0.5.0->dgl) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m801.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m977.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m811.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, dgl\n",
            "Successfully installed dgl-2.1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mục mới"
      ],
      "metadata": {
        "id": "K5w1YmgqlmxN"
      }
    }
  ]
}